{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "# useful stuff\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sentences(path, stopwords):\n",
    " # feel free to make a better tokenization/pre-processing\n",
    "    sentences = []\n",
    "    with open(path) as f:\n",
    "        for l in f:\n",
    "            sentences.append(clean_sentence(l, stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopwords(path):\n",
    "    #import stopwords file\n",
    "    stopwords_file = open(path, 'r')\n",
    "    stopwords = []\n",
    "    for word in stopwords_file:\n",
    "        stopwords.append(word.strip('\\n'))\n",
    "\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence, stopwords):\n",
    "    rx = re.compile('\\W+')\n",
    "    sentence = str(sentence).lower().split()\n",
    "    sentence = [rx.sub(' ',i).strip() for i in sentence if i not in stopwords and rx.sub(' ',i).strip()!= '']\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stopwords = '/home/louis/Documents/Data_science/YJ_PROJECT/Data/Data/stopwords.txt'\n",
    "stopwords = loadStopwords(path_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEMOCRATIE_ET_CITOYENNETE\n",
    "\n",
    "with open(\"/home/louis/Documents/Data_science/YJ_PROJECT/Data/Data/LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.json\") as json_file:\n",
    "    json_data = json.load(json_file) \n",
    "\n",
    "l=len(json_data) \n",
    "fiscalite_plus_juste_resp= open(\"fiscalite_plus_juste_resp.txt\", \"w+\")\n",
    "for i in range(l):\n",
    "    resp=json_data[i]['responses'][1]['value']\n",
    "    if resp!=None:\n",
    "        fiscalite_plus_juste_resp.write(resp)\n",
    "        fiscalite_plus_juste_resp.write(\"\\n\")\n",
    "\n",
    "fiscalite_plus_juste_resp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/home/louis/Documents/Data_science/YJ_PROJECT/Data/Data/fiscalite_plus_juste_resp.txt'\n",
    "tokenize_sentences=text2sentences(path_data,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_num={}\n",
    "i=0\n",
    "for sentence in tokenize_sentences:\n",
    "    for token in sentence:\n",
    "        if token not in word_2_num.keys():\n",
    "            i+=1\n",
    "            word_2_num[token]=str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_max = 100\n",
    "fiscalite_plus_juste_el = open(\"fiscalite_plus_juste_el.edgelist\", \"w+\")\n",
    "for sentence in tokenize_sentences:\n",
    "    if sentence:\n",
    "        for i,token in enumerate(sentence):\n",
    "            for neighbor in sentence[min(0,i-10):max(i+10,len(sentence))]:\n",
    "                if token!=neighbor:\n",
    "                    line=word_2_num[token]+\" \"+word_2_num[neighbor]\n",
    "                    fiscalite_plus_juste_el.write(line)\n",
    "                    fiscalite_plus_juste_el.write('\\n')\n",
    "fiscalite_plus_juste_el.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load deepwalk output from https://github.com/phanein/deepwalk\n",
    "folder='/home/louis/Documents/Data_science/YJ_PROJECT/deepwalk/'\n",
    "node=[]\n",
    "data_emb=[]\n",
    "with io.open(os.path.join(folder, \"fiscalite.embeddings\"), encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "          \n",
    "        node_, vec = line.rstrip().split(' ', 1)\n",
    "        \n",
    "        data_emb.append(np.fromstring(vec, sep=' ').tolist())\n",
    "        node.append(node_)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE on follower network\n",
    "\n",
    "### VERY LONG COMPUTING TIME , 60H ####\n",
    "%matplotlib inline\n",
    "pca= PCA(n_components=2, random_state=0)\n",
    "Y = pca.fit_transform(data_emb)\n",
    "\n",
    "x_coords = Y[:, 0]\n",
    "y_coords = Y[:, 1]\n",
    "# display scatter plot\n",
    "plt.scatter(x_coords, y_coords,s=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE on follower network\n",
    "\n",
    "### VERY LONG COMPUTING TIME , 60H ####\n",
    "%matplotlib inline\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "Y = tsne.fit_transform(data_emb)\n",
    "\n",
    "x_coords = Y[:, 0]\n",
    "y_coords = Y[:, 1]\n",
    "# display scatter plot\n",
    "plt.scatter(x_coords, y_coords,s=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
